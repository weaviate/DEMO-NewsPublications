{"keywords": ["systems", "machinelearning", "drugs", "answers", "automated", "costs", "data", "thinking", "debt", "unknown", "seller", "works", "hidden", "intellectual"], "paragraphs": ["Like many medications, the wakefulness drug modafinil, which is marketed under the trade name Provigil, comes with a small, tightly folded paper pamphlet. For the most part, its contents\u2014lists of instructions and precautions, a diagram of the drug\u2019s molecular structure\u2014make for anodyne reading. The subsection called \u201cMechanism of Action,\u201d however, contains a sentence that might induce sleeplessness by itself: \u201cThe mechanism(s) through which modafinil promotes wakefulness is unknown.\u201d Provigil isn\u2019t uniquely mysterious. Many drugs receive regulatory approval, and are widely prescribed, even though no one knows exactly how they work. This mystery is built into the process of drug discovery, which often proceeds by trial and error. Each year, any number of new substances are tested in cultured cells or animals; the best and safest of those are tried out in people. In some cases, the success of a drug promptly inspires new research that ends up explaining how it works\u2014but not always. Aspirin was discovered in 1897, and yet no one convincingly explained how it worked until 1995. The same phenomenon exists elsewhere in medicine. Deep-brain stimulation involves the implantation of electrodes in the brains of people who suffer from specific movement disorders, such as Parkinson\u2019s disease; it\u2019s been in widespread use for more than twenty years, and some think it should be employed for other purposes, including general cognitive enhancement. No one can say how it works. This approach to discovery\u2014answers first, explanations later\u2014accrues what I call intellectual debt. It\u2019s possible to discover what works without knowing why it works, and then to put that insight to use immediately, assuming that the underlying mechanism will be figured out later. In some cases, we pay off this intellectual debt quickly. But, in others, we let it compound, relying, for decades, on knowledge that\u2019s not fully known.", "In the past, intellectual debt has been confined to a few areas amenable to trial-and-error discovery, such as medicine. But that may be changing, as new techniques in artificial intelligence\u2014specifically, machine learning\u2014increase our collective intellectual credit line. Machine-learning systems work by identifying patterns in oceans of data. Using those patterns, they hazard answers to fuzzy, open-ended questions. Provide a neural network with labelled pictures of cats and other, non-feline objects, and it will learn to distinguish cats from everything else; give it access to medical records, and it can attempt to predict a new hospital patient\u2019s likelihood of dying. And yet, most machine-learning systems don\u2019t uncover causal mechanisms. They are statistical-correlation engines. They can\u2019t explain why they think some patients are more likely to die, because they don\u2019t \u201cthink\u201d in any colloquial sense of the word\u2014they only answer. As we begin to integrate their insights into our lives, we will, collectively, begin to rack up more and more intellectual debt. Theory-free advances in pharmaceuticals show us that, in some cases, intellectual debt can be indispensable. Millions of lives have been saved on the basis of interventions that we fundamentally do not understand, and we are the better for it. Few would refuse to take a life-saving drug\u2014or, for that matter, aspirin\u2014simply because no one knows how it works. But the accrual of intellectual debt has downsides. As drugs with unknown mechanisms of action proliferate, the number of tests required to uncover untoward interactions must scale exponentially. (If the principles by which the drugs worked were understood, bad interactions could be predicted in advance.) In practice, therefore, interactions are discovered after new drugs are on the market, contributing to a cycle in which drugs are introduced, then abandoned, with class-action lawsuits in between. In each individual case, accruing the intellectual debt associated with a new drug may be a reasonable idea. But intellectual debts don\u2019t exist in isolation. Answers without theory, found and deployed in different areas, can complicate one another in unpredictable ways. Intellectual debt accrued through machine learning features risks beyond the ones created through old-style trial and error. Because most machine-learning models cannot offer reasons for their ongoing judgments, there is no way to tell when they\u2019ve misfired if one doesn\u2019t already have an independent judgment about the answers they provide. Misfires can be rare in a well-trained system. But they can also be triggered intentionally by someone who knows just what kind of data to feed into that system. Consider image recognition. Ten years ago, computers couldn\u2019t easily identify objects in photos. Today, image search engines, like so many of the systems we interact with on a day-to-day basis, are based on extraordinarily capable machine-learning models. Google\u2019s image search relies on a neural network called Inception. In 2017, M.I.T.\u2019s LabSix\u2014a research group of undergraduate and graduate students\u2014succeeded in altering the pixels of a photograph of a cat so that, although it looked like a cat to human eyes, Inception became 99.99-per-cent sure it had been given a photograph of guacamole. (There was, it calculated, a slim chance that the photograph showed broccoli, or mortar.) Inception, of course, can\u2019t explain what features led it to conclude that a cat is a cat; as a result, there\u2019s no easy way to predict how it might fail when presented with specially crafted or corrupted data. Such a system is likely to have unknown gaps in its accuracy that amount to vulnerabilities for a smart and determined attacker. As knowledge generated by machine-learning systems is put to use, these kinds of gaps may prove consequential. Health-care A.I.s have been successfully trained to classify skin lesions as benign or malignant. And yet\u2014as a team of researchers from Harvard Medical School and M.I.T. showed, in a paper published this year\u2014they can also be tricked into making inaccurate judgments using the same techniques that turn cats into guacamole. (Among other things, attackers might use these vulnerabilities to commit insurance fraud.) Seduced by the predictive power of such systems, we may stand down the human judges whom they promise to replace. But they will remain susceptible to hijacking\u2014and we will have no easy process for validating the answers they continue to produce.", "Could we create a balance sheet for intellectual debt\u2014a system for tracking where and how theoryless knowledge is used? Our accounting could reflect the fact that not all intellectual debt is equally problematic. If an A.I. produces new pizza recipes, it may make sense to shut up and enjoy the pizza; by contrast, when we begin using A.I. to make health predictions and recommendations, we\u2019ll want to be fully informed. Building and maintaining a society-wide intellectual-debt balance sheet would probably require refining our approach to trade secrets and other intellectual property. In cities, building codes ask building owners to publicly disclose their renovation plans. Similarly, we might explore asking libraries or universities to accept, in escrow, otherwise hidden data sets and algorithms that have found a certain measure of public use. That would allow researchers to begin probing the models and underlying data on which we\u2019re coming to depend, and\u2014by building theories\u2014make payments on our intellectual debt before it becomes due in the form of errors and vulnerabilities. The growing pervasiveness of machine-learning models, and the fact that anyone can create one, promise to make this process of accounting difficult. But it\u2019s vital. Taken in isolation, oracular answers can generate consistently helpful results. But these systems won\u2019t stay in isolation: as A.I.s gather and ingest the world\u2019s data, they\u2019ll produce data of their own\u2014much of which will be taken up by still other systems. Just as drugs with unknown mechanisms of action sometimes interact, so, too, will debt-laden algorithms. Even simple interactions can lead to trouble. In 2011, a biologist named Michael Eisen found out, from one of his students, that the least-expensive copy of an otherwise unremarkable used book\u2014\u201cThe Making of a Fly: The Genetics of Animal Design\u201d\u2014was available on Amazon for $1.7 million, plus $3.99 shipping. The second-cheapest copy cost $2.1 million. The respective sellers were well established, with thousands of positive reviews between them. When Eisen visited the book\u2019s Amazon page several days in a row, he discovered that the prices were increasing continually, in a regular pattern. Seller A\u2019s price was consistently 99.83 per cent that of Seller B; Seller B\u2019s price was reset, every day, to 127.059 per cent of Seller A\u2019s. Eisen surmised that Seller A had a copy of the book, and was seeking to undercut the next-cheapest price. Seller B, meanwhile, didn\u2019t have a copy, and so priced the book higher; if someone purchased it, B could order it, on that customer\u2019s behalf, from A. Each seller\u2019s presumed strategy was rational. It was the interaction of their algorithms that produced irrational results. The interaction of thousands of machine-learning systems in the wild promises to be much more unpredictable. The financial markets, where cutting-edge machine-learning systems are already being deployed, provide an obvious breeding ground for this type of problem. In 2010, for a harrowing thirty-six minutes, a \u201cflash crash\u201d driven by algorithmic trading wiped more than a trillion dollars from the major U.S. indices. Last fall, the J. P. Morgan analyst Marko Kolanovic argued that such a crash could easily happen again, since more trading than ever is based on automated systems. Intellectual debt can accumulate in the interstices where systems bump into each other, even when they don\u2019t formally interconnect. Without anything resembling a balance sheet, there\u2019s no way to determine\u2014either in advance or retrospectively\u2014whether any particular quantity of intellectual debt is worth taking on."], "url": "https://www.newyorker.com/tech/annals-of-technology/the-hidden-costs-of-automated-thinking", "id": "caca0308-299a-3310-8f91-e2e31e7c6013", "title": "The Hidden Costs of Automated Thinking", "publicationId": "7e9b9ffe-e645-302d-9d94-517670623b35", "authors": ["Jonathan Zittrain", "Peter Schjeldahl", "Dexter Filkins", "Adam Entous", "Jia Tolentino", "Allison J. Pugh", "Kelly Clancy", "Tad Friend"], "summary": "In the past, intellectual debt has been confined to a few areas amenable to trial-and-error discovery, such as medicine.\nAs we begin to integrate their insights into our lives, we will, collectively, begin to rack up more and more intellectual debt.\nIn each individual case, accruing the intellectual debt associated with a new drug may be a reasonable idea.\nThe interaction of thousands of machine-learning systems in the wild promises to be much more unpredictable.\nThe financial markets, where cutting-edge machine-learning systems are already being deployed, provide an obvious breeding ground for this type of problem.", "pubDate": null}